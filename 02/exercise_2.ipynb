{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformations & Filtering (20 Pts)\n",
    "\n",
    "In this homework, we will explore some of the geometry that underlies how camera images are formed and how image filtering works."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Setup\n",
    "\n",
    "Generate the conda environment by running in the terminal:\n",
    "\n",
    "`conda create --name gcv_exercise_2 python=3.11`\n",
    "\n",
    "Run to activate environment:\n",
    "\n",
    "`conda activate gcv_exercise_2`\n",
    "\n",
    "Install all required packages for this exercise by running:\n",
    "\n",
    "`conda install matplotlib numpy scikit-image scikit-learn`\n",
    "\n",
    "Install OpenCV\n",
    "\n",
    "`conda install -c conda-forge opencv`\n",
    "\n",
    "Install Jupyter Notebook requirements:\n",
    "\n",
    "`conda install -n gcv_exercise_2 ipykernel --update-deps --force-reinstall`\n",
    "\n",
    "(Optional) If you are working on the TUWEL Juypter Notebooks:\n",
    "\n",
    "`ipython kernel install --name \"GCV_E_2\" --user`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Numpy is the main package for scientific computing with Python. \n",
    "# This will be one of our most used libraries in this class\n",
    "import numpy as np\n",
    "\n",
    "# The Time library helps us time code runtimes\n",
    "from time import time\n",
    "\n",
    "# Import the Scikit-Image library\n",
    "from skimage import io\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Matplotlib is a useful plotting library for python \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def hash_numpy(x):\n",
    "    import hashlib\n",
    "\n",
    "    return hashlib.sha1(x.view(np.uint8)).hexdigest()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Transformations in 3D\n",
    "\n",
    "In order to make sense of how objects in our world are rendered in a camera, we typically need to understand how they are located relative to the camera. In this question, we'll examine some properties of the transformations that formalize this process by expressing coordinates with respect to multiple frames. \n",
    "\n",
    "We'll be considering a scene with two frames: a world frame ($W$) and a camera frame ($C$).\n",
    "\n",
    "Notice that:\n",
    "- We have 3D points $p$, $q$, $r$, and $s$ that define a square, which is parallel to the world $zy$ plane\n",
    "- $C_z$ and $C_x$ belong to the plane defined by $W_z$ and $W_x$\n",
    "- $C_y$ is parallel to $W_y$\n",
    "\n",
    "<!-- into camera space as we translate a simple shape from world coordinates to camera coordinates. We will take this square in world coordinates and transform it into the camera coordinates.-->\n",
    "\n",
    "<img src=\"imgs/projection_geometry.png\" alt=\"projection geometry figure\" width=\"640\" />\n",
    "\n",
    "### Question 1.1: Reference Frame Definitions (3 Pts)\n",
    "\n",
    "First, we'll take a moment to validate your understanding of 3D reference frames.\n",
    "\n",
    "Consider creating:\n",
    "- A point $w$ at the origin of the world frame ($O_w$)\n",
    "- A point $c$ at the origin of the camera frame ($O_c$)\n",
    "\n",
    "Examine the $x$, $y$, and $z$ axes of each frame, then express these points with respect to the world and camera frames. Fill in **`w_wrt_camera`**, **`c_wrt_world`**, and **`c_wrt_camera`**.\n",
    "\n",
    "You can consider the length $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1.0\n",
    "\n",
    "# Abbreviation note:\n",
    "# - \"wrt\" stands for \"with respect to\", which is ~synonymous with \"relative to\"\n",
    "\n",
    "w_wrt_world = np.array([0.0, 0.0, 0.0])  # Done for you\n",
    "\n",
    "# *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "w_wrt_camera = np.array([0.0, 0.0, d])  # Assign me!\n",
    "\n",
    "c_wrt_world = np.array([1.0/np.sqrt(2.0), 0.0, 1.0/np.sqrt(2.0)])# Assign me!\n",
    "c_wrt_camera = np.array([0.0, 0.0, 0.0])  # Assign me!\n",
    "# *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to check your answers!\n",
    "assert (\n",
    "    (3,)\n",
    "    == w_wrt_world.shape\n",
    "    == w_wrt_camera.shape\n",
    "    == c_wrt_world.shape\n",
    "    == c_wrt_camera.shape\n",
    "), \"Wrong shape!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_world) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your w_wrt_world!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_camera) == \"6248a1dcfe0c8822ba52527f68f7f98955584277\"\n",
    "), \"Double check your w_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_camera) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your c_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_world) == \"a4c525cd853a072d96cade8b989a9eaf1e13ed3d\"\n",
    "), \"Double check your c_wrt_world!\"\n",
    "\n",
    "print(\"Looks correct!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: World â‡¨ Camera Transforms (3 Pts)\n",
    "\n",
    "Derive the homogeneous transformation matrix needed to convert a point expressed with respect to the world frame $W$ in the camera frame $C$.\n",
    "\n",
    "**Implement it in `camera_from_world_transform()`**.\n",
    "\n",
    "We've also supplied a set of `assert` statements below to help you check your work.\n",
    "\n",
    "---\n",
    "\n",
    "*Hint #1:*\n",
    "With rotation matrix $R \\in \\mathbb{R}^{3\\times 3}$ and translation vector $t \\in \\mathbb{R}^{3\\times 1}$, you can write transformations as $4 \\times 4$ matrices: \n",
    "$$\n",
    "\\begin{bmatrix}{x_C} \\\\ {y_C} \\\\ {z_C} \\\\ 1\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    \\vec{0}^\\top & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}{x_W} \\\\ {y_W} \\\\ {z_W} \\\\ 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Hint #2: Remember our 2D transformation matrix for rotations in the $xy$ plane.*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}{x}' \\\\ {y}'\\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y}\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "To apply this to 3D rotations, you might think of this $xy$ plane rotation as holding the $z$ coordinate constant, since that's the axis you're rotating around, and transforming the $x$ and $y$ coordinates as described in the 2D formulation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}{x}' \\\\ {y}' \\\\ {z}'\\end{bmatrix}  = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\ \\sin(\\theta) & \\cos(\\theta) & 0 \\\\ 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y} \\\\ {z}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(Alternatively you could simply take the rotation matrix from the [Wikipedia](https://en.wikipedia.org/wiki/Rotation_matrix) page)\n",
    "\n",
    "*Hint #3: In a homogeneous transform, the translation is applied after the rotation.*\n",
    "\n",
    "As a result, you can visualize the translation as an offset in the output frame.\n",
    "\n",
    "The order matters! You'll end up with a different transformation if you translate and then rotate versus if you rotate first and then translate with the same offsets. In the lecture we discussed a formulation for a combinated scaling, rotating, and translating matrix (in that order), which can be a useful starting point.\n",
    "\n",
    "\n",
    "*Hint #4: It might be useful to look up the connection between rotations and basis transfromation matrices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer against 1.1!\n",
    "def camera_from_world_transform(d: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Define a transformation matrix in homogeneous coordinates that\n",
    "    transforms coordinates from world space to camera space, according\n",
    "    to the coordinate systems in Question 1.\n",
    "    Args:\n",
    "        d (float, optional): Total distance of displacement between world and camera\n",
    ",    origins. Will always be greater than or equal to zero. Defaults to 1.0.\n",
    "    Returns:\n",
    "        T (np.ndarray): Left-hand transformation matrix, such that c = Tw\n",
    ",    for world coordinate w and camera coordinate c as column vectors.\n",
    ",    Shape = (4,4) where 4 means 3D+1 for homogeneous.\n",
    "    \"\"\"\n",
    "    T = np.eye(4)\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    assert T.shape == (4, 4)\n",
    "    return T\n",
    "\n",
    "T_camera_from_world = camera_from_world_transform()\n",
    "\n",
    "print(T_camera_from_world)\n",
    "\n",
    "# Check c_wrt_camera against T_camera_from_world @ w_wrt_world\n",
    "w_wrt_camera_computed = (T_camera_from_world @ np.append(w_wrt_world, 1.0))[:3]\n",
    "print(f\"w_wrt camera: expected {w_wrt_camera}, computed {w_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    w_wrt_camera, w_wrt_camera_computed\n",
    "), \"Error! (likely bad translation)\"\n",
    "print(\"Translation components look reasonable!\")\n",
    "\n",
    "# Check w_wrt_camera against T_camera_from_world @ c_wrt_world\n",
    "c_wrt_camera_computed = (T_camera_from_world @ np.append(c_wrt_world, 1.0))[:3]\n",
    "print(f\"c_wrt camera: expected {c_wrt_camera}, computed {c_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    c_wrt_camera, c_wrt_camera_computed\n",
    "), \"Error! (likely bad rotation)\"\n",
    "print(\"Rotation component looks reasonable!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filtering\n",
    "\n",
    "### Question 2.1: Convolution (2 Pts)\n",
    "\n",
    "Recall that the convolution of an image $f:\\mathbb{R}^2\\rightarrow \\mathbb{R}$ and a kernel $h:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is defined as follows:\n",
    "$$(f*h)[m,n]=\\sum_{i=-\\infty}^\\infty\\sum_{j=-\\infty}^\\infty f[i,j]\\cdot h[m-i,n-j]$$\n",
    "\n",
    "Or equivalently,\n",
    "\\begin{align}\n",
    "(f*h)[m,n] &= \\sum_{i=-\\infty}^\\infty\\sum_{j=-\\infty}^\\infty h[i,j]\\cdot f[m-i,n-j]\\\\\n",
    "&= (h*f)[m,n]\n",
    "\\end{align}\n",
    "\n",
    "In this section, you will implement:\n",
    "- `conv_nested`\n",
    "\n",
    "First, run the code cell below to load the image to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open image as grayscale\n",
    "img = io.imread('imgs/penguin.jpeg', as_gray=True)\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the function **`conv_nested`**. This is a naive implementation of convolution which uses 4 nested for-loops. It takes an image $f$ and a kernel $h$ as inputs and outputs the convolved image $(f*h)$ that has the same shape as the input image. This implementation should take a few seconds to run.\n",
    "\n",
    "*- Hint: It may be easier to implement $(h*f)$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_nested(image, kernel):\n",
    "    \"\"\"A naive implementation of convolution filter.\n",
    "    This is a naive implementation of convolution using 4 nested for-loops.\n",
    "    This function computes convolution of an image with a kernel and outputs\n",
    "    the result that has the same shape as the input image.\n",
    "    HINT: Look up `np.flip()` function.\n",
    "    Args:\n",
    "        image: numpy array of shape (Hi, Wi).\n",
    "        kernel: numpy array of shape (Hk, Wk). Dimensions will be odd.\n",
    "    Returns:\n",
    "        out: numpy array of shape (Hi, Wi).\n",
    "    \"\"\"\n",
    "    Hi, Wi = image.shape\n",
    "    Hk, Wk = kernel.shape\n",
    "    out = np.zeros((Hi, Wi))\n",
    "\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    return out\n",
    "\n",
    "# Simple convolution kernel.\n",
    "kernel = np.array(\n",
    "[\n",
    "    [1,2,1],\n",
    "    [0,0,0],\n",
    "    [-1,-2,-1]\n",
    "])\n",
    "\n",
    "# Create a test image: a white square in the middle\n",
    "test_img = np.zeros((9, 9))\n",
    "test_img[3:6, 3:6] = 1\n",
    "\n",
    "# Run your conv_nested function on the test image\n",
    "test_output = conv_nested(test_img, kernel)\n",
    "\n",
    "# Build the expected output\n",
    "expected_output = np.zeros((9, 9))\n",
    "expected_output[2:4, 2:3] = 1\n",
    "expected_output[2:4, 6:7] = 1\n",
    "expected_output[2:4, 3:4] = 3\n",
    "expected_output[2:4, 5:6] = 3\n",
    "expected_output[2:4, 4:5] = 4\n",
    "\n",
    "expected_output[5:7, 2:3] = -1\n",
    "expected_output[5:7, 6:7] = -1\n",
    "expected_output[5:7, 3:4] = -3\n",
    "expected_output[5:7, 5:6] = -3\n",
    "expected_output[5:7, 4:5] = -4\n",
    "\n",
    "# Plot the test image\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_img)\n",
    "plt.title('Test image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot your convolved image\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(test_output)\n",
    "plt.title('Convolution')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot the expected output\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(expected_output)\n",
    "plt.title('Expected output')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Test if the output matches expected output\n",
    "assert np.max(test_output - expected_output) < 1e-10, \"Your solution is not correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the kernel to see different outputs.\n",
    "\n",
    "out = conv_nested(img, kernel)\n",
    "\n",
    "# Plot original image\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot your convolved image\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(out)\n",
    "plt.title('Your convolution')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot what you should get\n",
    "solution_img = io.imread('imgs/convolved_penguin.jpeg')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(solution_img)\n",
    "plt.title('What you should get')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 Seperable Convolution Theory (1 Pts)\n",
    "\n",
    "Consider an $M_1\\times{N_1}$ image $I$ and an $M_2\\times{N_2}$ filter $F$. A filter $F$ is **separable** if it can be written as a product of two 1D filters: $F=F_1F_2$.\n",
    "\n",
    "For example,\n",
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "can be written as a matrix product of\n",
    "$$F_1=\n",
    "\\begin{bmatrix}\n",
    "1  \\\\\n",
    "1\n",
    "\\end{bmatrix},\n",
    "F_2=\n",
    "\\begin{bmatrix}\n",
    "1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Therefore $F$ is a separable filter.\n",
    "\n",
    "Consider an $M_1\\times{N_1}$ image $I$ and an $M_2\\times{N_2}$ filter $F$ that is separable (i.e. $F=F_1F_2$).\n",
    "\n",
    "(i) How many multiplication operations do you need to do a direct 2D convolution (i.e. $I*F$)?<br>\n",
    "(ii) How many multiplication operations do you need to do 1D convolutions on rows and columns (i.e. $(I*F_1)*F_2$)?<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:** *Write your solution in this markdown cell. Please write your equations in [LaTex equations](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Typesetting%20Equations.html).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will empirically compare the running time of a separable 2D convolution and its equivalent two 1D convolutions. The Gaussian kernel, widely used for blurring images, is one example of a separable filter. Run the code below to see its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5x5 Gaussian blur\n",
    "kernel = np.array([\n",
    "    [1,4,6,4,1],\n",
    "    [4,16,24,16,4],\n",
    "    [6,24,36,24,6],\n",
    "    [4,16,24,16,4],\n",
    "    [1,4,6,4,1]\n",
    "])\n",
    "\n",
    "t0 = time()\n",
    "out = conv_nested(img, kernel)\n",
    "t1 = time()\n",
    "t_normal = t1 - t0\n",
    "\n",
    "print(\"This took:\", t_normal,\"Seconds\")\n",
    "\n",
    "# Plot original image\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot convolved image\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(out)\n",
    "plt.title('Blurred')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 Seperable Convolution Application (2 Pts)\n",
    "\n",
    "In the below code cell, define the two 1D arrays (`k1` and `k2`) whose product is equal to the Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kernel can be written as outer product of two 1D filters\n",
    "k1 = None  # shape (5, 1)\n",
    "k2 = None  # shape (1, 5)\n",
    "\n",
    "# *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "pass\n",
    "# *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "# Check if kernel is product of k1 and k2\n",
    "if not  np.all(k1 * k2 == kernel):\n",
    "    print('k1 * k2 is not equal to kernel')\n",
    "    \n",
    "assert k1.shape == (5, 1), \"k1 should have shape (5, 1)\"\n",
    "assert k2.shape == (1, 5), \"k2 should have shape (1, 5)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the two versions of convolution to the same image, and compare their running time. Note that the outputs of the two convolutions must be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two convolutions using k1 and k2\n",
    "t0 = time()\n",
    "out_separable = conv_nested(img, k1)\n",
    "out_separable = conv_nested(out_separable, k2)\n",
    "t1 = time()\n",
    "t_separable = t1 - t0\n",
    "\n",
    "print(\"This took:\", t_separable,\"Seconds\")\n",
    "\n",
    "# Plot normal convolution image\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(out)\n",
    "plt.title('Normal convolution')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot separable convolution image\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(out_separable)\n",
    "plt.title('Separable convolution')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Normal convolution: took %f seconds.\" % (t_normal))\n",
    "print(\"Separable convolution: took %f seconds.\" % (t_separable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the two outputs are equal\n",
    "assert np.max(out_separable - out) < 1e-8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: SIFT Descriptor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a dataset called Fashion-MNIST. Fashion-MNIST is a dataset consisting of 60,000 examples of 28x28 greyscale images, associated with a label from 10 classes.\n",
    "\n",
    "<img src=\"imgs/fashion-mnist.png\" alt=\"Overview Fashion MNIST\" width=\"960\" />\n",
    "\n",
    "We first show the matching results, just using the raw images.\n",
    "The question we want to answer is: How can we improve the matching performance of the KNN-Matcher.\n",
    "\n",
    "We have implemented a simple descriptor function for you, where each keypoint is described by the normalized intensity of a small patch around it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    #Subset of Fashion-MNIST data\n",
    "\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Define the labels for each class\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "X_train, y_train = load_mnist('data/fashion', kind='train')\n",
    "\n",
    "### Select a subset of the data for faster runtime ###\n",
    "\n",
    "# Set the number of objects to select per class (also defines the nearest neighbours)\n",
    "k = 5\n",
    "\n",
    "# Get the unique class labels\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "# Initialize lists to store the selected features and labels\n",
    "selected_X = []\n",
    "selected_y = []\n",
    "\n",
    "# For each class label, randomly select x objects\n",
    "for c in classes:\n",
    "    # Get the indices of the objects with class label c\n",
    "    indices = np.where(y_train == c)[0]\n",
    "    \n",
    "    # Randomly select k objects\n",
    "    selected_indices = np.random.choice(indices, size=k, replace=False)\n",
    "    \n",
    "    # Add the selected features and labels to the corresponding lists\n",
    "    selected_X.append(X_train[selected_indices])\n",
    "    selected_y.append(y_train[selected_indices])\n",
    "    \n",
    "# Convert the selected features and labels to numpy arrays\n",
    "X_train = np.concatenate(selected_X)\n",
    "y_train = np.concatenate(selected_y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial matching using the raw images\n",
    "\n",
    "This code cell matches the images using the KNN implementation of SKLEARN, for now you do not need to understand the details about this matcher. The code plots the query image to the left (blue) and the five nearest neighbors after matching. You can run it multiple times, it always selects a new query, so results may vary. However, usually, for specific classes, the nearest neighbors are not all of the same class. Especially, the T-shirt/top and the Shirt class as well as all shoe related classes are often confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_train)\n",
    "\n",
    "# Plot the query images and their nearest neighbors\n",
    "fig, axs = plt.subplots(10, k, figsize=(10, 20))\n",
    "for label in range(10):\n",
    "    X = X_train[y_train == label]\n",
    "    idx = np.random.choice(X.shape[0], size=1, replace=False)\n",
    "    query = X[idx]\n",
    "    distances, indices = nbrs.kneighbors(query)\n",
    "\n",
    "    axs[label, 0].imshow(query.reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    axs[label, 0].set_title(label_names[label])\n",
    "    axs[label, 0].set_frame_on(True)\n",
    "    for axis in ['top', 'bottom', 'left', 'right']:\n",
    "        axs[label, 0].spines[axis].set_linewidth(3)  # change width\n",
    "        axs[label, 0].spines[axis].set_linestyle(':')\n",
    "        axs[label, 0].spines[axis].set_color('blue')\n",
    "        \n",
    "    #axs[label, 0].axis('off')    \n",
    "    for i in range(1, k):\n",
    "        \n",
    "        if y_train[indices[0][i]] != label:\n",
    "            # If there is a false positive, mark it with a red border\n",
    "            axs[label, i].imshow(X_train[indices[0][i]].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "            axs[label, i].set_frame_on(True)\n",
    "            axs[label, i].set_title(label_names[y_train[indices[0][i]]])\n",
    "            \n",
    "            for axis in ['top', 'bottom', 'left', 'right']:\n",
    "                axs[label, i].spines[axis].set_linewidth(3)  # change width\n",
    "                axs[label, i].spines[axis].set_linestyle('--')\n",
    "                axs[label, i].spines[axis].set_color('red')\n",
    "        else:\n",
    "            axs[label, i].imshow(X_train[indices[0][i]].reshape(28, 28), cmap='gray')\n",
    "            axs[label, i].set_frame_on(True)\n",
    "            for axis in ['top', 'bottom', 'left', 'right']:\n",
    "                axs[label, i].spines[axis].set_linewidth(3)  # change width\n",
    "                axs[label, i].spines[axis].set_color('green')\n",
    "        #axs[label, i].axis('off')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete analysis of the raw image matching\n",
    "\n",
    "This code cell plots the overall distribution of the 6000 nearest neighbors per class. As we have 6000 samples per class in the dataset, the 6000 nearest neighbors should all be of the same class. However, as you can see this is clearly not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_train)\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(30, 10))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for label in range(10):\n",
    "\n",
    "    X = X_train[y_train == label]\n",
    "    idx = np.random.choice(X.shape[0], size=1, replace=False)\n",
    "    query = X[idx]\n",
    "    distances, indices = nbrs.kneighbors(query)\n",
    "    name, occurance = np.unique(y_train[indices], return_counts=True)\n",
    "\n",
    "    # create an array of zeros with the length of the total number of classes\n",
    "    class_array = np.zeros(10)\n",
    "\n",
    "    # assign the count values to their respective class indices\n",
    "    for i in range(len(name)):\n",
    "        class_array[name[i]] = occurance[i]\n",
    "    axs[label].bar(label_names, class_array)\n",
    "    for label_ in axs[label].get_xticklabels():\n",
    "        label_.set_rotation(45)\n",
    "        label_.set_ha('right')\n",
    "    axs[label].set_xlabel('Class')\n",
    "    axs[label].set_ylabel('Matches')\n",
    "    axs[label].set_title(label_names[label])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 Computing a Descriptor (9 Pts)\n",
    "\n",
    "In this section, you will implement:\n",
    "- `compute_grad_mag_ori`\n",
    "- `compute_histogram_of_gradients`\n",
    "- `generate_descriptors_from_patches`\n",
    "\n",
    "`Note`: Your implementation will likely perform worse than the official OpenCV implementation, no points are deducted for that, as long as the 3 functions have been implemented correctly. You may add helper functions as required.\n",
    "\n",
    "#### Reference (OpenCV SIFT)\n",
    "The code below will serve as our reference, that we compare our implementation with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "imgs = X_train.reshape(k * len(classes), 28, 28)\n",
    "\n",
    "def get_center_kp(PS=65.):\n",
    "    c = PS/2.0\n",
    "    center_kp = cv2.KeyPoint()\n",
    "    center_kp.pt = (c,c)\n",
    "    center_kp.size = 2*c/5.303\n",
    "    return center_kp\n",
    "\n",
    "descriptors = []\n",
    "\n",
    "for img_id in range(k * len(classes)):\n",
    "    img = imgs[img_id, :, :]\n",
    "    img = cv2.resize(img, (16, 16), interpolation = cv2.INTER_AREA)\n",
    "    kp = get_center_kp(16)\n",
    "    kp,des = sift.compute(img, [kp])\n",
    "    descriptors.append(des)\n",
    "\n",
    "descriptors_np = np.array(descriptors)[:,0,:]\n",
    "print(descriptors_np.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete analysis of the OpenCV SIFT Descriptors image matching\n",
    "\n",
    "This code cell, again, plots the overall distribution of the 6000 nearest neighbors per class. However, this time it does not use the raw images as features, but instead uses the previously computed SIFT Descriptors as features for matching. As one can see the matching performance has improved, however, it is still not nearly perfect. To improve it further, we will use `Deep Learning` in a furture exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(descriptors_np)\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(30, 10))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for label in range(10):\n",
    "\n",
    "    X = descriptors_np[y_train == label]\n",
    "    idx = np.random.choice(X.shape[0], size=1, replace=False)\n",
    "    query = X[idx]\n",
    "    distances, indices = nbrs.kneighbors(query)\n",
    "    name, occurance = np.unique(y_train[indices], return_counts=True)\n",
    "\n",
    "    # create an array of zeros with the length of the total number of classes\n",
    "    class_array = np.zeros(10)\n",
    "\n",
    "    # assign the count values to their respective class indices\n",
    "    for i in range(len(name)):\n",
    "        class_array[name[i]] = occurance[i]\n",
    "    axs[label].bar(label_names, class_array)\n",
    "    for label_ in axs[label].get_xticklabels():\n",
    "        label_.set_rotation(45)\n",
    "        label_.set_ha('right')\n",
    "    axs[label].set_xlabel('Class')\n",
    "    axs[label].set_ylabel('Matches')\n",
    "    axs[label].set_title(label_names[label])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your implementation of the SIFT Descriptor\n",
    "\n",
    "Implement the `generate_descriptors_from_patches` function to compute a (simplified) SIFT descriptor for each image in the dataset. This function is used to generate a very unique fingerprint for the patch. To do this, we look at a 16 x 16 windows around the keypoint (as the Fashion-MNIST data is 28 x 28, it is resized to 16 x 16). This 16 x 16 window is then broken into sixteen 4x4 windows. \n",
    "\n",
    "<center><img src=\"imgs/sift-fingerprint.jpg\" alt=\"SIFT Fingerprint\" width=\"960\" /></center>\n",
    "\n",
    "Within each 4x4 window, gradient magnitudes and orientations are calculated. These orientations are put into an 8 bin histogram. \n",
    "\n",
    "<center><img src=\"imgs/sift-4x4.jpg\" alt=\"SIFT Gradients\" width=\"960\" /></center>\n",
    "\n",
    "Orientation in the range 0-44 degrees add to the first bin, 45-89 add to the next bin and so on. The amount added to the bin depends on the magnitude of the gradient. Unlike as in the past, the amount added also depends on the distance from the keypoint. So gradients that are far away from the keypoint will add smaller values to the histogram.\n",
    "\n",
    "This is done using a \"gaussian weighting function\". This function simply generates a gradient (it's like a 2D bell curve). You multiply it with the magnitude of orientations: The farther away, the lesser the magnutide. The sigma in the original paper is defined as one half of the width of the descriptor window. This weighting is put over the full patch with the keypoint (image patch center) as \"origin\" (see picture below, brown symbolizes the gaussian weighting function).\n",
    "\n",
    "<center><img src=\"imgs/sift-g_weighting.jpg\" alt=\"SIFT Gaussian Weighting\" width=\"360\" /></center>\n",
    "\n",
    "In the end you should end up with 4x4x8 = 128 numbers. Once you have all 128 numbers, you normalize them (just like you would normalize a vector in school, divide by root of sum of squares). These 128 numbers form the \"feature vector\". This keypoint is uniquely identified by this feature vector.\n",
    "\n",
    "This feature vector introduces a few complications. In the full version of SIFT you would get rid of them before finalizing the fingerprint.\n",
    "\n",
    "- Rotation dependence: The feature vector uses gradient orientations. Clearly, if you rotate the image, everything changes. All gradient orientations also change. To achieve rotation independence, the keypoint's rotation is subtracted from each orientation. Thus each gradient orientation is relative to the keypoint's orientation.\n",
    "- Illumination dependence: If we threshold numbers that are big, we can achieve achieve illumination independence. So, any number (of the 128) greater than 0.2 is changed to 0.2. This resultant feature vector is normalized again. And now you have an illumination independent feature vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_mag_ori(patch):\n",
    "    grad_mag, grad_ori = None, None\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    return grad_mag, grad_ori\n",
    "\n",
    "def compute_histogram_of_gradients(grad_ori, weight, nbins):\n",
    "    \"\"\"Calculate the histogram of gradients\"\"\"\n",
    "    \n",
    "    hist = None\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    return hist\n",
    "\n",
    "def generate_descriptors_from_patches(image_patch):\n",
    "    \"\"\"Generate descriptors for each patch\"\"\"\n",
    "    # TODO: Set the window_width correctly\n",
    "    window_width = None\n",
    "    desc = np.zeros((window_width*window_width*8,))\n",
    "\n",
    "\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    \n",
    "    for i in range(0, 16, window_width):\n",
    "        for j in range(0, 16, window_width):\n",
    "            # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "            pass\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "            \n",
    "    desc /= (np.linalg.norm(desc) + 1e-60)\n",
    "    desc[desc > 0.2] = 0.2\n",
    "    desc /= (np.linalg.norm(desc) + 1e-60)\n",
    "    return desc\n",
    "\n",
    "imgs = X_train.reshape(k * len(classes), 28, 28)\n",
    "\n",
    "descriptors = []\n",
    "\n",
    "for img_id in range(k * len(classes)):\n",
    "    img = imgs[img_id, :, :]\n",
    "    des = generate_descriptors_from_patches(img)\n",
    "    descriptors.append(des)\n",
    "\n",
    "descriptors_np = np.array(descriptors)\n",
    "print(descriptors_np.shape) #Should be (k * len(classes), 128)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(descriptors_np)\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(30, 10))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for label in range(10):\n",
    "\n",
    "    X = descriptors_np[y_train == label]\n",
    "    idx = np.random.choice(X.shape[0], size=1, replace=False)\n",
    "    query = X[idx]\n",
    "    distances, indices = nbrs.kneighbors(query)\n",
    "    name, occurance = np.unique(y_train[indices], return_counts=True)\n",
    "\n",
    "    # create an array of zeros with the length of the total number of classes\n",
    "    class_array = np.zeros(10)\n",
    "\n",
    "    # assign the count values to their respective class indices\n",
    "    for i in range(len(name)):\n",
    "        class_array[name[i]] = occurance[i]\n",
    "    axs[label].bar(label_names, class_array)\n",
    "    for label_ in axs[label].get_xticklabels():\n",
    "        label_.set_rotation(45)\n",
    "        label_.set_ha('right')\n",
    "    axs[label].set_xlabel('Class')\n",
    "    axs[label].set_ylabel('Matches')\n",
    "    axs[label].set_title(label_names[label])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
