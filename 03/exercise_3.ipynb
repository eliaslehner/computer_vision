{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: K-Means Clustering (15pts)\n",
    "\n",
    "In this homework, we will explore how the K-Nearest-Neighbour matching works."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Setup\n",
    "\n",
    "Generate the conda environment by running in the terminal:\n",
    "\n",
    "`conda create --name gcv_exercise_3 python=3.11`\n",
    "\n",
    "Run to activate environment:\n",
    "\n",
    "`conda activate gcv_exercise_3`\n",
    "\n",
    "Install all required packages for this exercise by running:\n",
    "\n",
    "`conda install matplotlib numpy scikit-image scikit-learn`\n",
    "\n",
    "Install Jupyter Notebook requirements:\n",
    "\n",
    "`conda install -n gcv_exercise_3 ipykernel --update-deps --force-reinstall`\n",
    "\n",
    "(Optional) If you are working on the TUWEL Juypter Notebooks:\n",
    "\n",
    "`ipython kernel install --name \"GCV_E_3\" --user`\n",
    "\n",
    "## Notebook Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-Means Clustering & Evaluation (10 Pts)\n",
    "\n",
    "### 1.1: K-Means Clustering (8pts)\n",
    "As discussed in class, K-Means is one of the most popular clustering algorithms. We have provided skeleton code for K-Means clustering in the `kmeans()`-function. Your task is to finish the implementation. We also provide you with a blob-dataset, that contains 1000 objects for 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data points for clustering\n",
    "n_samples = 1000\n",
    "centers = [[-2, 2], [2, 2], [0, 0], [-2, -2], [2, -2]]\n",
    "std = 0.6\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=std, random_state=42)\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(features, k, num_iters=100):\n",
    "    \"\"\" Use kmeans algorithm to group features into k clusters.\n",
    "    K-Means algorithm can be broken down into following steps:\n",
    "        1. Randomly initialize cluster centers\n",
    "        2. Assign each point to the closest center\n",
    "        3. Compute new center of each cluster\n",
    "        4. Stop if cluster assignments did not change\n",
    "        5. Go to step 2\n",
    "    Args:\n",
    "        features - Array of N features vectors. Each row represents a feature\n",
    "            vector.\n",
    "        k - Number of clusters to form.\n",
    "        num_iters - Maximum number of iterations the algorithm will run.\n",
    "    Returns:\n",
    "        assignments - Array representing cluster assignment of each point.\n",
    "            (e.g. i-th point is assigned to cluster assignments[i])\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = features.shape\n",
    "\n",
    "    assert N >= k, 'Number of clusters cannot be greater than number of points'\n",
    "\n",
    "    # Randomly initalize cluster centers\n",
    "    idxs = np.random.choice(N, size=k, replace=False)\n",
    "    centers = features[idxs]\n",
    "    assignments = np.zeros(N, dtype=np.uint32)\n",
    "\n",
    "    for n in range(num_iters):\n",
    "        # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "        pass\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "start = time()\n",
    "y_pred = kmeans(X, 5)\n",
    "end = time()\n",
    "\n",
    "kmeans_runtime = end - start\n",
    "\n",
    "print(\"kmeans running time: %f seconds.\" % kmeans_runtime)\n",
    "\n",
    "# Define a color and shape palette for plotting\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "shapes = ['o', 's', 'D', '^', 'v']\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(20, 20))\n",
    "\n",
    "y_pred_aligned = -np.ones_like(y_pred)\n",
    "for i in range(5):\n",
    "\n",
    "    # assign cluster id to class label\n",
    "    selected_objects = y_pred==i\n",
    "    class_ = np.argmax(np.bincount(y[selected_objects]))\n",
    "    y_pred_aligned[selected_objects] = class_\n",
    "\n",
    "    cluster_i = X[selected_objects]\n",
    "    axs.scatter(cluster_i[:, 0], cluster_i[:, 1], color=colors[i], marker=shapes[i], label=f'Cluster {i}')\n",
    "\n",
    "    # assign cluster id to class label\n",
    "    missclassifications = np.invert(np.logical_and(y_pred==i, y==class_))\n",
    "    class_missclassifications = np.logical_and(missclassifications, selected_objects)\n",
    "\n",
    "    cluster_fp = X[class_missclassifications]\n",
    "    axs.scatter(cluster_fp[:, 0], cluster_fp[:, 1], s=200, facecolors='none', edgecolors='black')\n",
    "\n",
    "axs.legend()\n",
    "axs.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Quantitative Evaluation (2 Pts)\n",
    "\n",
    "Looking at points is a good way to get an idea for how well an algorithm is working, but the best way to evaluate an algorithm is to have some quantitative measure of its performance. In this task you will quantitatively analyse the predictions of the k-means clustering. To do this we will first calculate the accuracy of the approach and then we will calculate the confusion matrix to see which class is most likely confused with which. \n",
    "\n",
    "<img src=\"imgs/example_confusion.png\" alt=\"Confusion Matrix\" width=\"800\" />\n",
    "\n",
    "Now implement the functions `compute_accuracy()` and `compute_confusion_matrix()`. For reference the example results are provided, yours should be close, but don't have to match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    \"\"\" Compute the accuracy of the clustering.\n",
    "    Args:\n",
    "        y_true - The ground truth. A array of size n_samples \n",
    "            where y_true[x] corresponds to the correct class for x.\n",
    "        y_pred - The prediction. A array of size n_samples \n",
    "            where y_pred[x] corresponds to the predicted class for x.\n",
    "    Returns:\n",
    "        accuracy - The fraction of samples where y_true and y_pred agree. A\n",
    "            bigger number is better, where 1.0 indicates a perfect clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = None\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\" Compute the confusion matrix for the clustering.\n",
    "    Args:\n",
    "        y_true - The ground truth. A array of size n_samples \n",
    "            where y_true[x] corresponds to the correct class for x.\n",
    "        y_pred - The prediction. A array of size n_samples \n",
    "            where y_pred[x] corresponds to the predicted class for x.\n",
    "    Returns:\n",
    "        confusion_matrix - array-like of shape (n_classes, n_classes)\n",
    "            The confusion matrix, where each element (i, j) represents the number of samples\n",
    "            that belong to class i but were predicted as class j. n_classes == len(centers)\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> y_true = [0, 1, 0, 1, 2, 2, 2]\n",
    "    >>> y_pred = [0, 1, 1, 1, 2, 0, 2]\n",
    "    >>> compute_confusion_matrix(y_true, y_pred)\n",
    "    array([[1, 1, 0],\n",
    "           [0, 2, 0],\n",
    "           [1, 0, 2]])\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix = None\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    pass\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = compute_accuracy(y, y_pred_aligned)\n",
    "cm = compute_confusion_matrix(y, y_pred_aligned)\n",
    "\n",
    "print(acc)\n",
    "print(cm)\n",
    "\n",
    "# 0.989\n",
    "# [[198   0   2   0   0]\n",
    "#  [  0 198   2   0   0]\n",
    "#  [  1   0 195   2   2]\n",
    "#  [  0   0   0 200   0]\n",
    "#  [  0   0   2   0 198]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: k-Means for CIFAR-10 (5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will apply your k-means algorithm to the CIFAR-10 dataset. [Download](https://www.cs.toronto.edu/~kriz/cifar.html) and extract the *Python* version of the CIFAR10 dataset into a folder *outside* of the code folder. Read the website to understand which classes there are and how the data are structured. The code for loading train and test splits is given below; simply change the path to your data directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for loading the CIFAR10 datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_image(row):\n",
    "  data = np.reshape(row, (32, 32, 3), order='F')  # row to 32x32x3\n",
    "  data = np.transpose(data, (1, 0, 2))  # transpose x and y axis of image\n",
    "  return data\n",
    "\n",
    "\n",
    "def load_CIFAR10_batch(batch_path):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  try:\n",
    "    with open(str(batch_path), 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "        data_images = data_dict[b'data'] # 10000 x 3072\n",
    "\n",
    "        for i, label in enumerate(data_dict[b'labels']):\n",
    "\n",
    "            X_list.append(format_image(data_images[i]))\n",
    "            Y_list.append(label)\n",
    "\n",
    "        return np.stack(X_list), np.stack(Y_list)\n",
    "  except:\n",
    "    output = f'{batch_path} is not a directory or an expected file inside is missing.'\n",
    "    raise ValueError(output)\n",
    "  \n",
    "\n",
    "def load_CIFAR10(data_path, n_per_class: int = None, shuffle: bool = False):\n",
    "\n",
    "  X, Y= load_CIFAR10_batch(Path(data_path) / 'test_batch')\n",
    "  if n_per_class is not None:\n",
    "     X_list = []\n",
    "     Y_list = []\n",
    "     for i in range(10):\n",
    "        mask = Y==i\n",
    "        assert n_per_class <= mask.sum()\n",
    "        X_list.append(X[mask][:n_per_class])\n",
    "        Y_list.append(Y[mask][:n_per_class])\n",
    "        \n",
    "     X = np.concatenate(X_list)\n",
    "     Y = np.concatenate(Y_list)\n",
    "  \n",
    "  if shuffle:\n",
    "    idx_shuffled = np.array(range(X.shape[0]))\n",
    "    np.random.shuffle(idx_shuffled)\n",
    "    X = X[idx_shuffled]\n",
    "    Y = Y[idx_shuffled]\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `cifar10_dir` variable to your path, where you stored the CIFAR10 dataset and use the functions above to load images and their label from the CIFAR10 dataset.\n",
    "\n",
    "We only use images from the *test_batch* of the CIFAR10 dataset as you can see in the function above. We are working with a balanced dataset since for each class we sample the same amount of samples. The number of samples per class can be specified with `n_per_class`. If you load 200 images per class the output of the following cell should be this:\n",
    "```\n",
    "Data shape:  (2000, 32, 32, 3)\n",
    "Labels shape:  (2000,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dir = ... # TODO\n",
    "X, Y= ... # TODO\n",
    "\n",
    "# Checking the size of the training and testing data\n",
    "print('Data shape: ', X.shape)\n",
    "print('Labels shape: ', Y.shape)\n",
    "\n",
    "\n",
    "# Data shape:  (2000, 32, 32, 3)\n",
    "# Labels shape:  (2000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the CIFAR10 dataset\n",
    "\n",
    "The image produced by the following code cell should look like this (note that the class images can vary since we select the indices for the plot randomly):\n",
    "\n",
    "<img src=\"imgs/cifar10.png\" alt=\"Gini Index\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(Y == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the k-Means algorithm\n",
    "\n",
    "The input feature vectors to our k-Means algorithm are the flattened images, meaning that each pixel is stacked on top of eachother. \n",
    "The shape of the dataset is then\n",
    "\n",
    "```\n",
    "Data shape: (2000, 3072)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# reshaping data and placing into rows - stack pixels onto eachother\n",
    "X_flat = ... # TODO\n",
    "print(f'Data shape: {X_flat.shape}')\n",
    "\n",
    "\n",
    "# Data shape: (2000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(input_feats, k=10, num_iters=200):\n",
    "\n",
    "    Y_pred = ... # TODO - use the kmeans function from part 1\n",
    "\n",
    "    # map cluster ids to class ids\n",
    "    Y_pred_aligned = -np.ones_like(Y_pred)\n",
    "    for i in range(10):\n",
    "        selected_objects = Y_pred==i\n",
    "        class_ = np.argmax(np.bincount(Y[selected_objects]))\n",
    "        Y_pred_aligned[selected_objects] = class_\n",
    "    return Y_pred_aligned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `apply_kmeans` function to compare the clustering results on CIFAR10 using the raw inputs vs. the standardized inputs. \n",
    "\n",
    "The results for 200 interations should be something close to:\n",
    "```\n",
    "Accuracy - raw: 0.146\n",
    "Accuracy - std: 0.2275\n",
    "```\n",
    "You see that standadization makes a big difference however the results are not great in general. Feel free to try different number of iterations. \n",
    "It seems that simply stacking pixels on top of each other are not good features. In the next exercise (exercise 4) we will start with Neural Networks which are great feature extractors that can be trained and learn in contrast to raw features or hand crafted features you learnt in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 200\n",
    "# Standardize the input features, do not use functions from python library\n",
    "X_std = ... # TODO\n",
    "\n",
    "\n",
    "pred_raw = apply_kmeans(X_flat, k=10, num_iters=iterations)\n",
    "acc_raw = compute_accuracy(Y, pred_raw)\n",
    "print(f\"Accuracy - raw: {acc_raw}\")\n",
    "\n",
    "pred_std = apply_kmeans(X_std, k=10, num_iters=iterations)\n",
    "acc_std = compute_accuracy(Y, pred_std)\n",
    "print(f\"Accuracy - std: {acc_std}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcv_exercise_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4100d1fd6af48ceef52f0dd79a10481ac076cdc74c14cfe98be479f321db18d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
