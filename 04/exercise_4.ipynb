{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Theory of NNs (20pts)\n",
    "\n",
    "In this homework, we will explore the basics of neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Setup\n",
    "\n",
    "Generate the conda environment by running in the terminal:\n",
    "\n",
    "`conda create --name gcv_exercise_4 python=3.11`\n",
    "\n",
    "Run to activate environment:\n",
    "\n",
    "`conda activate gcv_exercise_4`\n",
    "\n",
    "Install all required packages for this exercise by running:\n",
    "\n",
    "`conda install matplotlib numpy scikit-image scikit-learn tqdm`\n",
    "\n",
    "Install Jupyter Notebook requirements:\n",
    "\n",
    "`conda install -n gcv_exercise_4 ipykernel --update-deps --force-reinstall`\n",
    "\n",
    "(Optional) If you are working on the TUWEL Juypter Notebooks:\n",
    "\n",
    "`ipython kernel install --name \"GCV_E_4\" --user`\n",
    "\n",
    "## Notebook Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# The Random module implements pseudo-random number generators\n",
    "import math\n",
    "import random \n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "# Numpy is the main package for scientific computing with Python. \n",
    "# This will be one of our most used libraries in this class\n",
    "import numpy as np\n",
    "\n",
    "# Import the Scikit-Image and -Learn library\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from skimage.transform import resize\n",
    "from PIL import Image \n",
    "# Matplotlib is a useful plotting library for python \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Perceptron (8 Pts)\n",
    "\n",
    "The perceptron is one of the simplest and oldest algorithms in machine learning. It is a type of artificial neural network that was introduced by Frank Rosenblatt. The perceptron is used for binary classification problems where the goal is to separate two classes of data using a linear boundary. The algorithm updates the weights of the inputs to adjust the boundary until the correct classification is achieved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use a similar dataset to exercise 3, however, as the perceptron can only seperate 2 classes we will only use 2 centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data points for clustering\n",
    "n_samples = 800\n",
    "centers = [[-1, 1], [1, -1]]\n",
    "std = 0.8\n",
    "\n",
    "#generate dataset\n",
    "X_in, y_in = make_blobs(n_samples=n_samples, centers=centers, cluster_std=std, random_state=42)\n",
    "\n",
    "\n",
    "#split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_in, y_in, test_size=0.3, stratify=y_in, random_state=42)\n",
    "# Plot data points\n",
    "plt.scatter(X_in[:, 0], X_in[:, 1], c=y_in, s=50, cmap='viridis')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Implementation\n",
    "\n",
    "Here you will implement the:\n",
    "\n",
    "- Perceptron (`predict()`)\n",
    "- Perceptron weight update (`fit()`)\n",
    "\n",
    "Function Description (`predict()`)\n",
    "\n",
    "The predict function should implement the perceptron algorithm for predicting the class labels of input vectors. The function takes as input a numpy ndarray X of shape (n_samples, n_features) and uses the weight vector self.w to compute the output of the perceptron for each sample. The output of the perceptron for each sample is computed as the dot product of the sample and the weight vector self.w, then the bias term self.bias is added. Finally, the function applies a step function to the output to obtain the predicted class label for each sample. The step function returns 0 for all values less than or equal to zero and 1 for all values greater than zero. The predicted class labels should be returned as a numpy ndarray of shape (n_samples,).\n",
    "\n",
    "Function Description (`fit()`)\n",
    "\n",
    "The fit function should implement the training algorithm for the single layer perceptron. The function initializes the weight vector self.w and bias term self.bias with zeros and then loops through the specified number of iterations (epochs). In each epoch, the function calculates the predictions for all samples in X using the prediction method, and then compares them with the ground truth labels y to calculate the prediction error. The function should update the weights and bias using the perceptron learning rule. After each epoch, the function appends the number of misclassifications to a list miss_classifications. Finally, the function should return the miss_classifications list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Implements the single layer perceptron.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, n_iterations=1000):\n",
    "        \"\"\"Initialize perceptron with learning rate and epochs.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "            n_iterations (int): Number of training epochs\n",
    "            \n",
    "        Attributes:\n",
    "            weights (numpy.ndarray): Weights of the perceptron\n",
    "            bias (int): Bias value of the perceptron\n",
    "\n",
    "        \"\"\"\n",
    "        self.learning_rate = lr\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Training function.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Inputs.\n",
    "            y (numpy.ndarray): labels/target.\n",
    "\n",
    "        Returns:\n",
    "            List of the number of miss-classifications per epoch\n",
    "        \"\"\"\n",
    "        # Initialize weights and bias\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        miss_class = []\n",
    "\n",
    "        for _ in tqdm(range(self.n_iterations)):\n",
    "            # Calculate predictions for all examples at once\n",
    "            predictions = self.predict(X)\n",
    "\n",
    "            # Update weights and bias based on prediction errors\n",
    "            errors = y - predictions\n",
    "            # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "            self.weights += ...\n",
    "            self.bias += ...\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "            \n",
    "            miss_class.append((errors!=0).sum())\n",
    "        return miss_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Prediction function.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Inputs.\n",
    "\n",
    "        Returns:\n",
    "            Class label of X\n",
    "        \"\"\"\n",
    "        \n",
    "        # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "        # Calculate the dot product of weights and input features, add bias\n",
    "        z = ...\n",
    "        # Apply the step function to get the binary predictions\n",
    "        return ...\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Training and Visualization\n",
    "\n",
    "You don't need to change any code here. The code should produce a plot similar to this:\n",
    "\n",
    "<img src=\"imgs/example_output_perc.png\" alt=\"Plot\" width=\"960\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(lr=0.01)\n",
    "\n",
    "miss_list = perceptron.fit(X_train, y_train)\n",
    "\n",
    "y_pred = perceptron.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(cm)\n",
    "acc = accuracy_score(y_val,y_pred)\n",
    "\n",
    "print(\"Accuracy on the testset:\", acc)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(30, 10))\n",
    "\n",
    "axs[0].plot(miss_list)\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Missclassifications')\n",
    "axs[0].set_title('Missclassifications during training')\n",
    "\n",
    "# Create grid of points covering entire feature space\n",
    "xx, yy = np.meshgrid(np.arange(X_in[:,0].min()-1, X_in[:,0].max()+1, 0.1),\n",
    "                     np.arange(X_in[:,1].min()-1, X_in[:,1].max()+1, 0.1))\n",
    "\n",
    "# Apply trained perceptron to each point on the grid\n",
    "Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "axs[1].contourf(xx, yy, Z, alpha=0.4)\n",
    "axs[1].scatter(X_in[:, 0], X_in[:, 1], c=y_in, alpha=0.8, s=50, cmap='viridis')\n",
    "axs[1].set_xlabel('Feature 1')\n",
    "axs[1].set_ylabel('Feature 2')\n",
    "axs[1].set_title('Decision Boundary (on Train- + Testset)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Convolutional Neural Network (12 Pts)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and activation function\n",
    "\n",
    "__Loss Function__:\n",
    "A loss function, also known as a cost function or objective function, is a measure of how well a machine learning model is performing on a given task. The goal of a machine learning algorithm is to minimize the value of the loss function, which is achieved by adjusting the model's parameters during training. Different types of machine learning problems require different types of loss functions. For example, for a classification task, a common loss function is cross-entropy loss, which measures the difference between the predicted probability distribution and the true distribution of labels. For a regression task, mean squared error (MSE) is a common loss function, which measures the average squared difference between the predicted values and the true values.\n",
    "\n",
    "__Activation Function__:\n",
    "An activation function is a mathematical function that is applied to the output of each neuron in a neural network. The purpose of the activation function is to introduce nonlinearity into the model, which allows it to learn more complex patterns in the data. Some common activation functions include the sigmoid function, which maps the input to a value between 0 and 1, and the ReLU (rectified linear unit) function, which returns the input if it is positive and 0 otherwise. Different activation functions can be used in different layers of a neural network, depending on the requirements of the specific task and the architecture of the network. You will implement the sigmoid loss function and its derivative for this lecture.\n",
    "\n",
    "__Loss functions are out of scope for this exercise and are provided__."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <title>My Page</title>\n",
    "  <style>\n",
    "    figure {\n",
    "      margin: 0;\n",
    "      padding: 0;\n",
    "    }\n",
    "\n",
    "    img {\n",
    "      display: block;\n",
    "      margin: auto;\n",
    "    }\n",
    "\n",
    "    figcaption {\n",
    "      font-size: 25px;\n",
    "      color: #666;\n",
    "      text-align: center;\n",
    "      margin-top: 10px;\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "<div style=\"background-color: white;\">\n",
    "  <figure>\n",
    "    <center><img src=\"imgs/sigmoid.svg\" alt=\"Plot\" width=\"960\" /></center>\n",
    "    <figcaption>Sigmoid function</figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareLoss(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "    def delta(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "    def calculate_accuracy(self, y_pred, y):\n",
    "        y_pred = y_pred[:, 0]\n",
    "        assert y.ndim == 1 and y.size == y_pred.size\n",
    "        y_pred = y_pred > 0.5\n",
    "        return (y == y_pred).sum().item() / y.size\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the output of the Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    return \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)*****\n",
    "    return \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "You already implemented basic (handcrafted) convolutions in `exercise 2`, now you will implement learned convolutions with a forward pass and a backward pass.In exercise 2, you learned how to perform basic convolutions using pre-defined filters or kernels. In this exercise, you will take it a step further and implement learned convolutions, where the model learns the filters or kernels during training instead of using pre-defined ones.\n",
    "\n",
    "To do this, you will implement a backward pass for the learned convolution layer. The code for the forward pass is given; it involves taking the input data and passing it through the convolution layer, which applies the learned filters to the input and produces an output feature map. The backward pass involves calculating the gradients of the loss with respect to the input and the parameters of the convolution layer, which allows the weights to be updated during training using backpropagation.\n",
    "\n",
    "By implementing learned convolutions, the model can learn to extract more complex features from the input data, which can lead to better performance on the task at hand. However, this also means that there are more parameters to learn and optimize during training, which can be computationally expensive and require a lot of training data. \n",
    "\n",
    "In this exercise, you won't actually be training the model to convergence. Instead, you will perform a single forward pass and backward pass through the learned convolution layer, and compare the resulting gradients to the expected gradients that are provided to you. This will allow you to verify that your implementation of the convolution layer is correct and that the gradients are being computed accurately. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Part of the Backwardpass for the Convolution-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional:\n",
    "    \"\"\"\n",
    "    A class representing a convolutional layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name: str\n",
    "        The name of the convolutional layer.\n",
    "    stride: int, default=1\n",
    "        The stride of the convolutional layer.\n",
    "    size: int, default=3\n",
    "        The size of the filters in the convolutional layer.\n",
    "    activation: str or None, default=None\n",
    "        The activation function to apply after the convolution operation.\n",
    "    last_input: numpy array or None\n",
    "        The input to the last forward pass through the layer.\n",
    "    sigmoid: numpy vectorize function\n",
    "        A vectorized implementation of the sigmoid activation function.\n",
    "    sigmoid_derivative: numpy vectorize function\n",
    "        A vectorized implementation of the derivative of the sigmoid activation function.\n",
    "    filters: numpy array\n",
    "        The filters for the convolutional layer.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    forward(image):\n",
    "        Perform a forward pass through the convolutional layer.\n",
    "    backward(din, learn_rate):\n",
    "        Perform a backward pass through the convolutional layer.\n",
    "    get_weights():\n",
    "        Return the weights of the convolutional layer.\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> conv = Convolutional(name='conv_layer')\n",
    "    >>> conv.forward(np.random.randn(32, 28, 28))\n",
    "    array([[[...], [...], ... , [...]], [[...], [...], ... , [...]], ... , [[...], [...], ... , [...]]])\n",
    "    >>> conv.backward(np.random.randn(16, 26, 26), learn_rate=0.001)\n",
    "    array([[[...], [...], ... , [...]], [[...], [...], ... , [...]], ... , [[...], [...], ... , [...]]])\n",
    "    >>> conv.get_weights()\n",
    "    array([...])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, in_channels=16, out_channels=8, stride=1, size=3, activation=False):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Convolutional class.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the convolutional layer.\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels (filters).\n",
    "            stride (int): The stride used for the convolution.\n",
    "            size (int): The size of the filters.\n",
    "            activation (bool): Defines if the sigmoid activation function is applied.).\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        limit = 1 / math.sqrt(in_channels)\n",
    "        np.random.seed(42)\n",
    "        self.filters = np.random.uniform(-limit, limit, (out_channels, in_channels, size, size))\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        self.last_input = None\n",
    "        self.sigmoid = np.vectorize(sigmoid)\n",
    "        self.sigmoid_derivative = np.vectorize(sigmoid_derivative)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Applies the convolution operation to the input image and returns the output.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        image : numpy.ndarray of shape (n_channels, input_dimension, input_dimension)\n",
    "            The input image to the convolutional layer.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        out : numpy.ndarray of shape (n_filters, output_dimension, output_dimension)\n",
    "            The output of the convolution operation after applying filters to the input image.\n",
    "        \"\"\"\n",
    "        self.last_input = image                             # keep track of last input for later backward propagation\n",
    "\n",
    "        input_dimension = image.shape[1]                                                # input dimension\n",
    "        output_dimension = int((input_dimension - self.size) / self.stride) + 1         # output dimension\n",
    "\n",
    "        out = np.zeros((self.filters.shape[0], output_dimension, output_dimension))     # create the matrix to hold the\n",
    "                                                                                        # values of the convolution\n",
    "        for f in range(self.filters.shape[0]):              # convolve each filter over the image,\n",
    "            tmp_y = out_y = 0                               # moving it vertically first and then horizontally\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = image[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    out[f, out_y, out_x] += np.sum(self.filters[f] * patch)\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        # print(out)\n",
    "        if self.activation:                       # apply Sigmoid activation function\n",
    "            return self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, din, learn_rate=0.005):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through the convolutional layer and returns the loss gradient \n",
    "        for this layer's inputs.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        din : numpy.ndarray of shape (n_filters, output_dimension, output_dimension)\n",
    "            The loss gradient from the layer above.\n",
    "        learn_rate : float, optional\n",
    "            The learning rate to be used during gradient descent.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dout : numpy.ndarray of shape (n_channels, input_dimension, input_dimension)\n",
    "            The loss gradient for this layer's inputs.\n",
    "        \"\"\"\n",
    "        print(\"din shape: \", din.shape)\n",
    "        input_dimension = self.last_input.shape[1]          # input dimension\n",
    "\n",
    "        if self.activation:                                 # back propagate through sigmoid\n",
    "           din = self.sigmoid_derivative(din)\n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)              # loss gradient of the input to the convolution operation\n",
    "        dfilt = np.zeros(self.filters.shape)                # loss gradient of filter\n",
    "\n",
    "        for f in range(self.filters.shape[0]):              # loop through all filters\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    # *****BEGINNING OF YOUR CODE (DO NOT DELETE THIS LINE)***** # get the patch from last input\n",
    "                    pass                                                         # compute the loss gradient of filter\n",
    "                    # *****END OF YOUR CODE (DO NOT DELETE THIS LINE)*****       # compute the loss gradient of the input to the convolution operation\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        self.filters -= learn_rate * dfilt                  # update filters using SGD\n",
    "        return dout                                         # return the loss gradient for this layer's inputs\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns the filters of the convolutional layer as a 1D numpy array.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        weights : numpy.ndarray of shape (n_filters * n_channels * filter_size * filter_size)\n",
    "            The filters of the convolutional layer, flattened into a 1D array.\n",
    "        \"\"\"\n",
    "        return np.reshape(self.filters, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN \"Training\"\n",
    "\n",
    "You don't need to change any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = SquareLoss()\n",
    "\n",
    "# Load example image, convert it to numpy and normalize to 0 and 1\n",
    "im = np.array(Image.open(\"imgs/Sneaker.png\"))/255\n",
    "\n",
    "# Create a one-hot encoding for example image\n",
    "# ['Sandal', 'Shirt', 'Sneaker', 'Ankle boot']\n",
    "one_hot_label = [0, 0, 1, 0]\n",
    "\n",
    "layers = []\n",
    "gradients = []\n",
    "\n",
    "layers.append(Convolutional(name='conv1', in_num_filters=1, out_num_filters=8, stride=2, size=3, activation=True))\n",
    "layers.append(Convolutional(name='conv2', in_num_filters=8, out_num_filters=8, stride=2, size=3, activation=True))\n",
    "layers.append(Convolutional(name='conv3', in_num_filters=8, out_num_filters=4, stride=2, size=6, activation=True))\n",
    "\n",
    "im.resize(28, 28, 1)\n",
    "print(\"Initial image shape:\", im.shape)\n",
    "\n",
    "# Output:\n",
    "# Initial image shape: (28, 28, 1)\n",
    "# Image shape after convolution: (8, 13, 13)\n",
    "# Image shape after convolution: (8, 6, 6)\n",
    "# Image shape after convolution: (4, 1, 1)\n",
    "\n",
    "# We need to switch the channels here [H, W, C] --> [C, H, W], this is the standard notation in PyTorch.\n",
    "image = np.transpose(im, [2,0,1])\n",
    "for layer in layers:\n",
    "    image = layer.forward(image)\n",
    "    print(\"Image shape after convolution:\", image.shape)\n",
    "\n",
    "prediction = image.flatten()\n",
    "\n",
    "gradient = loss.delta(one_hot_label, prediction)[:, np.newaxis, np.newaxis]\n",
    "gradients.append(gradient)\n",
    "for layer in reversed(layers):\n",
    "    gradient = layer.backward(gradient, 0.01)\n",
    "    gradients.append(gradient)\n",
    "\n",
    "\n",
    "# Output:\n",
    "# conv3: (4, 1, 1)\n",
    "# con2: (8, 6, 6)\n",
    "# conv1: (8, 13, 13)\n",
    "# Input: (1, 28, 28)\n",
    "for gradient_ in gradients:\n",
    "    print(gradient_.shape)\n",
    "    print(np.mean(gradient_))\n",
    "\n",
    "assert (\n",
    "    np.allclose(np.mean(gradients[0]), 0.39115646115584257)\n",
    "), \"Check convolution backpropagation\"\n",
    "assert (\n",
    "    np.allclose(np.mean(gradients[1]), 0.03910490241680205)\n",
    "), \"Check convolution backpropagation\"\n",
    "assert (\n",
    "    np.allclose(np.mean(gradients[2]), -0.010947803994090859)\n",
    "), \"Check convolution backpropagation\"\n",
    "assert (\n",
    "    np.allclose(np.mean(gradients[3]), 0.1281080665962364)\n",
    "), \"Check convolution backpropagation\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise_4_gcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e987157f1f655ef20b37f4412fd1225dc750396d67b88c63783233eca5c75510"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
